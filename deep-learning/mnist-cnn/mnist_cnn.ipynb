{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer (Kaggle), 0.991 Accuracy with Keras\n",
    "<hr>\n",
    "In this tutorial we are going to use __*Convolutional Neural Networks*__ to classify images from the __*MNIST*__ dataset.\n",
    "\n",
    "- You can find the competition [here](https://www.kaggle.com/c/digit-recognizer/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "%pylab inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, MaxPool2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import numpy.random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the dataset\n",
    "After downloading the dataset, we are going to do the following:\n",
    "\n",
    "1. Open the file and load the data\n",
    "2. Format the data and get the labels\n",
    "3. Check for NaN values\n",
    "4. Split the dataset to train and validation\n",
    "5. Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_train_data(path):\n",
    "    \n",
    "    train = [] \n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        lines = list(reader)\n",
    "        for line in tqdm(lines[1:]):\n",
    "            label = line[0]\n",
    "            \n",
    "            image = np.array([x for x in line[1:]])\n",
    "            image = image.astype('float32')\n",
    "            \n",
    "            # Format the data to 28x28x1 (in grey scale)\n",
    "            image = np.reshape(image, (28, 28, 1))\n",
    "            train.append([image, label])\n",
    "    \n",
    "    return np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(train):\n",
    "    \n",
    "    np.random.shuffle(train)\n",
    "    \n",
    "    features = [x[0] for x in train]\n",
    "    labels = [x[1] for x in train]\n",
    "    \n",
    "    # Split the dataset to train and validation\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.025, random_state=42)\n",
    "    \n",
    "    # One-hot Encoding\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    return (np.array(x_train), y_train), (np.array(x_test), y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, run only once\n",
    "# train = open_train_data('dataset/train.csv')\n",
    "# np.save('train.npy', train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have already ran the the function open_train_data then run this\n",
    "train = np.load('train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "import pandas as pd\n",
    "\n",
    "for idx, feature in enumerate(train):\n",
    "    if pd.isnull(feature).any():\n",
    "        print('Found NaN value in feature %d' % idx)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = split_train_test(train)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40950, 28, 28, 1), (40950, 10), (1050, 28, 28, 1), (1050, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (2, 2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 1)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(256, (2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# model.add(Conv2D(128, (2, 2), padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "#                  activation ='relu', input_shape = (28,28,1)))\n",
    "# model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "#                  activation ='relu'))\n",
    "# model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation = \"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        160       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 27, 32)        4128      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 27, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 27, 27, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 27, 27, 64)        8256      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 128)       32896     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 6, 6, 256)         131328    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,390,410\n",
      "Trainable params: 1,390,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=40,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "batch_size = 64\n",
    "\n",
    "opt_rms = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# opt_rms = keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt_rms, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.3599 - acc: 0.9129 - val_loss: 0.0890 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96857, saving model to model-001.h5\n",
      "Epoch 2/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.2426 - acc: 0.9408 - val_loss: 0.1124 - val_acc: 0.9676\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.96857\n",
      "Epoch 3/50\n",
      "640/640 [==============================] - 18s 29ms/step - loss: 0.2356 - acc: 0.9430 - val_loss: 0.1163 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.96857 to 0.97714, saving model to model-003.h5\n",
      "Epoch 4/50\n",
      "640/640 [==============================] - 19s 29ms/step - loss: 0.2301 - acc: 0.9462 - val_loss: 0.0833 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.97714\n",
      "Epoch 5/50\n",
      "640/640 [==============================] - 18s 29ms/step - loss: 0.2191 - acc: 0.9472 - val_loss: 0.0869 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.97714\n",
      "Epoch 6/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2334 - acc: 0.9448 - val_loss: 0.0870 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.97714 to 0.97714, saving model to model-006.h5\n",
      "Epoch 7/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.2307 - acc: 0.9443 - val_loss: 0.0641 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.97714 to 0.98286, saving model to model-007.h5\n",
      "Epoch 8/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.2259 - acc: 0.9456 - val_loss: 0.1015 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.98286\n",
      "Epoch 9/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.2430 - acc: 0.9441 - val_loss: 0.1008 - val_acc: 0.9676\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.98286\n",
      "Epoch 10/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2448 - acc: 0.9430 - val_loss: 0.1921 - val_acc: 0.9571\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.98286\n",
      "Epoch 11/50\n",
      "640/640 [==============================] - 21s 32ms/step - loss: 0.2453 - acc: 0.9431 - val_loss: 0.0939 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.98286\n",
      "Epoch 12/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.2495 - acc: 0.9416 - val_loss: 0.0655 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.98286 to 0.98286, saving model to model-012.h5\n",
      "Epoch 13/50\n",
      "640/640 [==============================] - 19s 29ms/step - loss: 0.2480 - acc: 0.9425 - val_loss: 0.1475 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.98286\n",
      "Epoch 14/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2526 - acc: 0.9419 - val_loss: 0.0950 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.98286\n",
      "Epoch 15/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2577 - acc: 0.9402 - val_loss: 0.0685 - val_acc: 0.9867\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.98286 to 0.98667, saving model to model-015.h5\n",
      "Epoch 16/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2590 - acc: 0.9392 - val_loss: 0.3358 - val_acc: 0.9552\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.98667\n",
      "Epoch 17/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2774 - acc: 0.9366 - val_loss: 0.1068 - val_acc: 0.9714\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.98667\n",
      "Epoch 18/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2767 - acc: 0.9369 - val_loss: 0.0744 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.98667\n",
      "Epoch 19/50\n",
      "640/640 [==============================] - 21s 32ms/step - loss: 0.2767 - acc: 0.9364 - val_loss: 0.0778 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.98667\n",
      "Epoch 20/50\n",
      "640/640 [==============================] - 21s 33ms/step - loss: 0.2839 - acc: 0.9337 - val_loss: 0.0892 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.98667\n",
      "Epoch 21/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2743 - acc: 0.9364 - val_loss: 0.0886 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.98667\n",
      "Epoch 22/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.2910 - acc: 0.9328 - val_loss: 0.0939 - val_acc: 0.9790\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.98667\n",
      "Epoch 23/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.2885 - acc: 0.9330 - val_loss: 0.1288 - val_acc: 0.9695\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.98667\n",
      "Epoch 24/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.3007 - acc: 0.9307 - val_loss: 0.1099 - val_acc: 0.9762\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.98667\n",
      "Epoch 25/50\n",
      "640/640 [==============================] - 20s 32ms/step - loss: 0.2993 - acc: 0.9299 - val_loss: 0.1092 - val_acc: 0.9714\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.98667\n",
      "Epoch 26/50\n",
      "640/640 [==============================] - 21s 32ms/step - loss: 0.3143 - acc: 0.9274 - val_loss: 0.1148 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.98667\n",
      "Epoch 27/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.3102 - acc: 0.9262 - val_loss: 0.1118 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.98667\n",
      "Epoch 28/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.3288 - acc: 0.9237 - val_loss: 0.1121 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.98667\n",
      "Epoch 29/50\n",
      "640/640 [==============================] - 19s 29ms/step - loss: 0.3232 - acc: 0.9249 - val_loss: 0.0971 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.98667\n",
      "Epoch 30/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.3206 - acc: 0.9258 - val_loss: 0.1032 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.98667\n",
      "Epoch 31/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.3231 - acc: 0.9256 - val_loss: 0.1520 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.98667\n",
      "Epoch 32/50\n",
      "640/640 [==============================] - 19s 30ms/step - loss: 0.3381 - acc: 0.9224 - val_loss: 0.1077 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.98667\n",
      "Epoch 33/50\n",
      "640/640 [==============================] - 20s 31ms/step - loss: 0.3264 - acc: 0.9216 - val_loss: 0.1373 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.98667\n",
      "Epoch 34/50\n",
      "640/640 [==============================] - 20s 32ms/step - loss: 0.3456 - acc: 0.9195 - val_loss: 0.0955 - val_acc: 0.9714\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.98667\n",
      "Epoch 35/50\n",
      "440/640 [===================>..........] - ETA: 6s - loss: 0.3396 - acc: 0.9214"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "epochs = 50\n",
    "\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph/{}'.format(time()), histogram_freq=0, write_graph=True, write_images=True)\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', verbose=1, monitor='val_acc', save_best_only=True, mode='auto')\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tbCallBack, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model-035.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "def open_test_data(path):\n",
    "    \n",
    "    test = [] \n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        lines = list(reader)\n",
    "        image_number = 1\n",
    "        for line in tqdm(lines[1:]):\n",
    "            \n",
    "            image = np.array([x for x in line])\n",
    "            image = image.astype('float32')\n",
    "            image = np.reshape(image, (28, 28, 1))\n",
    "            test.append([image, image_number])\n",
    "            image_number += 1\n",
    "    \n",
    "    return np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28000/28000 [00:10<00:00, 2552.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# test_data = open_test_data('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('test.npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28000/28000 [00:52<00:00, 534.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('ImageId,Label\\n')\n",
    "    for data in tqdm(test_data):\n",
    "        arr = numpy.expand_dims(data[0], axis=0)\n",
    "        number = model.predict(arr)\n",
    "        \n",
    "        label = argmax(number)\n",
    "        f.write(str(data[1]) + ',' + str(label) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tensorflow-gpu)",
   "language": "python",
   "name": "envname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
